---
title: "Exploratory Data Analysis"
subtitle: "PSTAT 131"
author: "Isabella Escamilla"
date: "3/18/23"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    df_print: paged
---

<style type = "text/css">
  body{
  font-family: trebuchet ms, sans-serif;
}
.list-group-item.active, .list-group-item.active:focus, .list-group-item.active:hover {
    background-color: #8A5667;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	eval = TRUE,
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
library(tidyverse)
library(ggplot2)
library(extrafont)
library(showtext)
library(gt)
library(grid)
library(gridExtra)
library(tidymodels)
library(xgboost)
library(kknn)
library(doParallel)
registerDoParallel(cores = parallel::detectCores())
```

```{r, include=FALSE}
loadfonts(device='win', quiet = FALSE)
font_add('trebuchet', 'C:/windows/fonts/trebuc.ttf')
showtext_auto()
```

```{r, include=FALSE}
theme_polyvore <- function(){
  font <- 'trebuchet'
  
  theme_minimal() %+replace%
    
    theme(
      # grid elements
      #panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      axis.ticks = element_blank(),
      axis.line = element_line(),
      
      # text elements
      plot.title = element_text(
        family = font,
        size = 28,
        face = 'bold',
        hjust = 0,
        vjust = 2,
        margin = margin(b=5, t=5)),
      
      plot.subtitle = element_text(          #subtitle
                   family = font,            #font family
                   size = 14),               #font size
      
      plot.caption = element_text(           #caption
                   family = font,            #font family
                   size = 9,                 #font size
                   hjust = 1),               #right align
      
      axis.title = element_text(             #axis titles
                   family = font,            #font family
                   size = 20),               #font size
      
      axis.text = element_text(              #axis text
                   family = font,            #axis family
                   size = 16),                #font size
      
      axis.text.x = element_text(            #margin for axis text
                    margin=margin(5, b = 10)),
      
      axis.title.y = element_text(margin=margin(b=10, r=10), angle=90)
      

    )
}
```


## Exploratory Data Analysis
Now that we've tidied the data in a format that will be better suited for our analysis, we can begin by making some plots and seeing how the data is distributed.

First we will import the data and convert the appropriate variables into factors.

```{r}
# Reading the tidied data into R
outfits <- read.csv('tidy_data.csv')

# converting all character columns into factors
outfits[sapply(outfits, is.character)] <- lapply(outfits[sapply(outfits, is.character)], as.factor)
# I will only be using set_id to retrieve photos, so we will leave it be

outfits %>% head(5)
```


In the final data set, there are 14 variables:

Name | Variable description
---|---
`views` | The number of times an outfit has been viewed
`likes` | The number of likes an outfit received **(the response variable)**
`set_id` | identifies the particular outfit
`item_1` - `item_8` | gives the name of the item type for up to 8 items in an outfit
`color_1` - `color_4` | gives the unique colors found in the outfit for up to 4 different colors


### Likes
We will start by looking at the distribution of our response variable, `likes`.


```{r}
outfits %>% 
  ggplot(aes(x = likes)) + 
  geom_histogram(binwidth = 100, fill="#2C514C") +
  labs(x = 'Number of Likes', y = 'Count', title = 'Distribution of the Number of Likes') +
  coord_cartesian(xlim = c(0, 4000)) +
  theme_polyvore()
```

The number of likes ranges from 0 to around 3000, and the frequency decreases exponentially as the number of likes goes up. This follows the sort of logic you'd expect, where a select few get lots of likes while most go unseen.

### Items (Clothing Articles)
After an initial survey of the raw data, I noted that there were a lot of different item ids.

```{r, class.source = 'fold-show'}
# Finds the number of unique items in item_1 column
outfits$item_1 %>% unique() %>% length()
```
And indeed, there are **229 unique items** in `item_1` alone! To further examine the nature of the item columns, we will look at a plot of the item frequencies in `item_1`. Additionally, we'll do a "zoom in" of the top 10 items by frequency in `item_1`.

```{r}
outfits %>% ggplot(aes(x = fct_infreq(item_1))) +
  geom_bar(fill = '#F08080') +
  labs(x = 'Item 1', y='', title = 'Frequencies of item_1 Items') +
  theme_polyvore() +
  theme(axis.text.x = element_blank(), 
        panel.grid.major.x = element_blank(), 
        axis.line = element_blank()) -> p1

item1_t10 <- (sort(table(outfits$item_1), decreasing = TRUE) %>% names)[1:10]
outfits_item1_top10 <- outfits[outfits$item_1 %in% item1_t10,]

outfits_item1_top10 %>% 
  ggplot(aes(y = fct_infreq(item_1))) +
  geom_bar(fill="#583742") +
  labs(x = '', y='', title = 'Top 10 item_1 Items') +
  theme_polyvore() +
  theme(axis.line = element_blank()) -> p2

grid.arrange(p1, p2, ncol=2)
```

The first plot shows that occurrences of the 229 items in `item_1` are concentrated towards the top few items and fall off significantly after. We can see from the second plot that the top items of `item_1` are all tops, dresses, and jackets of some type. This similarity gives us an idea of how the items have been organized, and we can get a better picture from constructing a table.

```{r}
item1_t5 <- (sort(table(outfits$item_1), decreasing = TRUE) %>% names)[1:5]
item2_t5 <- (sort(table(outfits$item_2), decreasing = TRUE) %>% names)[1:5]
item3_t5 <- (sort(table(outfits$item_3), decreasing = TRUE) %>% names)[1:5]
item4_t5 <- (sort(table(outfits$item_4), decreasing = TRUE) %>% names)[1:5]
item5_t5 <- (sort(table(outfits$item_5), decreasing = TRUE) %>% names)[1:5]
item6_t5 <- (sort(table(outfits$item_6), decreasing = TRUE) %>% names)[1:5]
item7_t5 <- (sort(table(outfits$item_7), decreasing = TRUE) %>% names)[1:5]
item8_t5 <- (sort(table(outfits$item_8), decreasing = TRUE) %>% names)[1:5]

item_freq_table <- data.frame(item_1 = item1_t5,
                              item_2 = item2_t5,
                              item_3 = item3_t5,
                              item_4 = item4_t5,
                              item_5 = item5_t5,
                              item_6 = item6_t5,
                              item_7 = item7_t5,
                              item_8 = item8_t5)

```

```{r}
item_freq_table %>% gt() %>% 
  tab_header(
    title = "Top 5 Items by Frequency"
  )
```

From looking at both the raw data and this table, a general pattern emerges: within each outfit, the items are sorted from most to least significant, with `item_1` being the most significant item and `item_8` being the least. The top items in `item_2` are still significant, with jackets being the most frequent and "bottoms" being common elements as well (shorts, jeans, skirts). `item_3` dips into shoe territory, and `item_4` through `item_8` can be seen mostly as the "accessory section". Another thing that is interesting to note from the table is that from `item_5` and onward, "None" is the most frequent item. We can check the proportion of "None"s pretty easily.

```{r}
# Gets proportion of data that has fewer than 5 items, fewer than 6 items, fewer than 7 items, and fewer than 8 items
props <- c(((filter(outfits, item_4 == 'None') %>% count()) / (outfits %>% count()))[1,1] %>% as.numeric(),
((filter(outfits, item_5 == 'None') %>% count()) / (outfits %>% count()))[1,1] %>% as.numeric(),
((filter(outfits, item_6 == 'None') %>% count()) / (outfits %>% count()))[1,1] %>% as.numeric(),
((filter(outfits, item_7 == 'None') %>% count()) / (outfits %>% count()))[1,1] %>% as.numeric(),
((filter(outfits, item_8 == 'None') %>% count()) / (outfits %>% count()))[1,1] %>% as.numeric())
props
```

With some simple arithmetic, this gives us a breakdown of the item number proportions:

* **8%** of the outfits have 4 items,
* **15%** have 5 items,
* **17%** have 6 items,
* **19%** have 7 items, and
* **41%** have 8 items

Now we want to see what relationship the items have with likes. Before doing so, I will transform the data so that we only have 10 different categories, which will be the top 9 occurrences plus another category, 'other'.

```{r}
outfits2 <- outfits %>% mutate(item_1 = fct_lump_n(outfits$item_1, n=9))
colors <- c('#335C57', '#583742', '#F08080',
             '#335C57', '#583742', '#F08080',
             '#335C57', '#583742', '#F08080',
             '#335C57')
             
outfits2 %>% 
  ggplot(aes(x=likes, y=item_1, group=item_1)) +
  geom_boxplot(fill = colors) +
  labs(y = '', title = 'Boxplot of item_1') +
  theme_polyvore() +
  theme(axis.title.y = element_blank()) -> p1

outfits2 %>% 
  ggplot(aes(x=likes, y=item_1, group=item_1)) +
  geom_boxplot(fill = colors) +
  scale_x_continuous(limits = c(0, 600)) +
  labs(y = '', title = 'Boxplot of item_1 (zoomed)') +
  theme_polyvore() +
  theme(axis.title.y = element_blank()) -> p2

grid.arrange(p1, p2, ncol = 2)
```

In the first boxplot, we can see the full range of likes. It's hard to glean any information from the boxes themselves, but we can see differences in the spread of outliers for each category. In the second boxplot, I've "zoomed in" by limiting the x axis so we can see the boxes better. We can see here that sweaters and day dresses have the highest average number of likes, while tank tops trail behind all of the other categories.

### Colors
All of us understand that color plays an essential role in whether or not an outfit "works" (unless you're doing wardrobe for a 1947 film noir), so it will be interesting to see what kind of colors are represented in the data.

```{r}
colors1 <- c('#fb5c5c', '#4a687d', '#ffdfbf',
            'gray16', '#9b938c', '#e2ba59', '#628d55',
            '#643b34', '#e3774c', '#6e4258')

outfits %>% 
  ggplot(aes(y = fct_infreq(color_1))) +
  geom_bar(fill = colors1) +
  labs(x = '', y = '', title = 'Frequencies of Colors in color_1') +
  theme_polyvore() +
  theme(axis.line = element_blank(),
        panel.grid.major = element_blank()) -> p1

colors2 <- c('gray16', '#4e4c5e', '#ffdfbf', '#e2ba59',
             '#9b938c', '#643b34', '#4a687d', '#628d55',
             '#fb5c5c', '#e3774c', '#6e4258')

outfits %>% 
  ggplot(aes(y = fct_infreq(color_2))) +
  geom_bar(fill = colors2) +
  labs(x = '', y = '', title = 'Frequencies of Colors in color_2') +
  theme_polyvore() +
  theme(axis.line = element_blank(),
        panel.grid.major = element_blank()) -> p2

colors3 <- c('#4e4c5e', 'gray16', '#ffdfbf', '#e2ba59',
             '#9b938c', '#643b34', '#4a687d', '#6e4258',
             '#fb5c5c', '#628d55', '#e3774c')

outfits %>% 
  ggplot(aes(y = fct_infreq(color_3))) +
  geom_bar(fill = colors3) +
  labs(x = '', y = '', title = 'Frequencies of Colors in color_3') +
  theme_polyvore() +
  theme(axis.line = element_blank(),
        panel.grid.major = element_blank()) -> p3

colors4 <- c('#4e4c5e', 'gray16', '#9b938c', '#fb5c5c',
             '#e2ba59', '#4a687d', '#ffdfbf', '#6e4258',
             '#e3774c', '#643b34', '#628d55')

outfits %>% 
  ggplot(aes(y = fct_infreq(color_4))) +
  geom_bar(fill = colors4) +
  labs(x = '', y = '', title = 'Frequencies of Colors in color_4') +
  theme_polyvore() +
  theme(axis.line = element_blank(),
        panel.grid.major = element_blank()) -> p4

grid.arrange(p1, p2, p3, p4, ncol=2)

```

The color columns are similar to the item columns in the way they are ordered: `color_1` will have higher importance than `color_2` (and so on) in the outfit due to the way the information was extracted. This gives us some insight into the colors represented in these charts.

Red clearly reigns supreme for `color_1`, followed by blue, white, and black. We start to see "none" dominate starting in `color_2`, indicating that there are lots of outfits for which only one color could be extracted (or simply had a monochromatic scheme going on). In `color_4`, we see that most of the outfits simply don't have a color entry. This could mean that most outfits stuck to 1-3 colors.

Next, we'll take a look at the relationship between color and likes. 

```{r}
colors <- c('gray16', '#4a687d', '#643b34',
            '#9b938c', '#628d55', '#e3774c', '#6e4258',
            '#fb5c5c', '#ffdfbf', '#e2ba59')

outfits %>% 
  ggplot(aes(x=likes, y=color_1, group=color_1)) +
  geom_boxplot(fill=colors) +
  labs(title = 'Boxplot of color_1') +
  theme_polyvore() +
  theme(axis.title.y = element_blank()) -> p1

outfits %>% 
  ggplot(aes(x=likes, y=color_1, group=color_1)) +
  geom_boxplot(fill=colors) +
  scale_x_continuous(limits = c(0, 500)) + 
  labs(title = 'Boxplot of color_1 (zoomed)') +
  theme_polyvore() + 
  theme(axis.title.y = element_blank()) -> p2

grid.arrange(p1, p2, ncol=2)
```

The left graph shows similar behavior as the `item_1` boxplot, with all of the colors being concentrated near zero but having different spreads of outliers into the higher numbers of likes. On the right boxplot, we can see that orange has the highest number of average likes, while red and blue are towards the low end. This is interesting since red and blue were the most frequent colors and orange was the second least frequent, so this could potentially mean that being less common gives outfits with orange a slight edge.

## Fitting Models
With the information gathered from the exploratory data analysis, we can start fitting models. There are a few key steps in fitting a model:

* Split the data into training and testing
* Specify a recipe for the model to use
* Create a workflow for the model
* Do any tuning necessary
* Fit training data to the model(s)
* Fit best model to the testing data based off of training performance metrics

Regression models will be used for this case because the response variable, `likes`, is numeric (we also have an interesting situation where all of our predictors are categorical). Because there are so many different categories in the item columns, this could lead to a problem of overfitting. For now, we will keep each of these columns to 10 categories.

 ### Splitting the Data

Without further ado, we will start by splitting our data into training and testing. I've decided to split it into 70% training and 30% testing, stratifying on the `likes` variable. This ensures that the training and testing have the same distribution of likes.

```{r, lass.source = 'fold-show'}
# Setting each item category to the top 9 categories, the rest will be "other"
outfits <- outfits %>% 
  mutate(item_1 = fct_lump_n(outfits$item_1, n=9), 
         item_2 = fct_lump_n(outfits$item_2, n=9), 
         item_3 = fct_lump_n(outfits$item_3, n=9), 
         item_4 = fct_lump_n(outfits$item_4, n=9), 
         item_5 = fct_lump_n(outfits$item_5, n=9), 
         item_6 = fct_lump_n(outfits$item_6, n=9), 
         item_7 = fct_lump_n(outfits$item_7, n=9), 
         item_8 = fct_lump_n(outfits$item_8, n=9))

set.seed(222)
outfits_split <- initial_split(outfits, prop=.70,
                               strata=likes)
outfits_train <- training(outfits_split)
outfits_test <- testing(outfits_split)
```


### Creating a Recipe
The next step is to make a recipe that our models will use to fit the data. In our case, this is pretty analogous to constructing an outfit. Which colors should I include and how many? Should I wear a coat? What about boots? We can get this information from `item_1` through `item_8` and `color_1` through `color_4`. I'm excluding `views` because even though it's probably a good predictor, it isn't really something you consider when picking out an outfit. 

```{r, class.source = 'fold-show'}
outfit_recipe <- recipe(likes ~ item_1 + item_2 + item_3 + item_4 +
                          item_5 + item_6 + item_7 + item_8 +
                          color_1 + color_2 + color_3 + color_4,
                        data = outfits) %>% 
    step_dummy(all_nominal_predictors()) %>% 
    step_interact(terms = ~ starts_with("color_1"):starts_with("color_2"))
```

I've also decided to include an interaction term between `color_1` and `color_2` to represent the "color scheme" of the outfit. This will allow the model to consider, for example, if an outfit has both red and green, or if it has black and white, and so on.

### K-Fold Cross Validation
For our project, we will be using k-fold cross validation with a *k* value of 3, which is a process that splits up the training data into k different groups and then chooses one as the validation group, then fits the model using the rest of the groups as training, repeating the process *k* times. This process is useful since it produces less variance in the estimates of the model's performance.

```{r, class.source = 'fold-show'}
set.seed(222)
outfits_fold <- vfold_cv(outfits_train, v=10)
```

### Model Building
The models that we will be using for this data, as mentioned before, will be regression models. The following are the ones I've chosen to include:

* Linear regression
* Elastic net 
* K-nearest neighbors
* Random Forest
* Boosted Trees

First, we will set up the models.

```{r, class.source = 'fold-show'}
# Linear Model
lm_model <- linear_reg() %>% 
  set_engine("lm")


# K-nearest neighbors, tuning neighbors
knn_model <- nearest_neighbor(neighbors = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("kknn")


# Elastic net, tuning penalty and mixture
elastic_model <- linear_reg(penalty = tune(), 
                           mixture = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet")


# Random forest, tuning mtry, trees, and min_n
rf_model <- rand_forest(mtry = tune(), 
                       trees = tune(), 
                       min_n = tune()) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("regression")


# Boosted trees, tuning trees, learn_rate, and mtry
boosted_model <- boost_tree(trees = tune(),
                           learn_rate = tune(),
                           mtry = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
```

Next, we will set up the workflows for each model.

```{r, class.source = 'fold-show'}
lm_wflow <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(outfit_recipe)


knn_wflow <- workflow() %>% 
  add_model(knn_model) %>% 
  add_recipe(outfit_recipe)


elastic_wflow <- workflow() %>% 
  add_recipe(outfit_recipe) %>% 
  add_model(elastic_model)


rf_wflow <- workflow() %>% 
  add_recipe(outfit_recipe) %>% 
  add_model(rf_model)


boosted_wflow <- workflow() %>% 
  add_recipe(outfit_recipe) %>% 
  add_model(boosted_model)
```

Now we'll make tuning grids for the models with tuning parameters.

```{r, class.source = 'fold-show'}
knn_grid <- grid_regular(neighbors(range = c(5,20)), levels = 5)


elastic_grid <- grid_regular(penalty(),
                        mixture(range = c(0, 1)),
                             levels = 10)


rf_grid <- grid_regular(mtry(range = c(2, 4)), 
                        trees(range = c(10, 100)),
                        min_n(range = c(1, 20)),
                        levels = 8)

boosted_grid <- grid_regular(mtry(range = c(2, 4)), 
                        trees(range = c(10, 100)),
                        learn_rate(range = c(-10, -1)),
                        levels = 5)
```

And now the part where my PC takes a minor beating: tuning.

```{r, class.source = 'fold-show', eval=FALSE}
knn_tune <- tune_grid(
    knn_wflow,
    resamples = outfits_fold,
    grid = knn_grid
)


elastic_tune <- tune_grid(
  elastic_wflow,
  resamples = outfits_fold,
  grid = elastic_grid
)


rf_tune <- tune_grid(
  rf_wflow,
  resamples = outfits_fold,
  grid = rf_grid
)


boosted_tune <- tune_grid(
  boosted_wflow,
  resamples = outfits_fold,
  grid = boosted_grid
)
```

6 minutes and 39 seconds! Not bad! Now, I'll save the results and load them back in so I don't have to donate another 6 minutes and 39 seconds of my future.

```{r, eval=FALSE}
save(knn_tune, file = "tunings/knn_tune.rda")
save(elastic_tune, file = "tunings/elastic_tune.rda")
save(rf_tune, file = "tunings/rf_tune.rda")
save(boosted_tune, file = "tunings/boosted_tune.rda")
```

```{r}
load("tunings/knn_tune.rda")
load("tunings/elastic_tune.rda")
load("tunings/rf_tune.rda")
load("tunings/boosted_tune.rda")
```

**BAM!** Models are tuned!* We now want to find the root mean square error (RMSE) for each model. The RMSE is a measure of the differences between predicted values and actual values, so this will be useful in comparing the models' performances. 

\* Linear regression hasn't been tuned since there it doesn't have tuning parameters... I bet he feels so alone right now... It's okay though. He will still be fit.

```{r, eval=FALSE}
lm_fit <- fit_resamples(lm_wflow, resamples = outfits_fold)
save(lm_fit, file = "tunings/lm_fit.rda")
```

Now we can look at the performance of the model on the training data. First, we'll look at the RMSE and then we'll look at the autoplots of the tuned model.

```{r}
load("tunings/lm_fit.rda")

lm_rmse <- collect_metrics(lm_fit)$mean[1] %>% round(2)

knn_rmse <- show_best(knn_tune, n=1)$mean %>% round(2)

elastic_rmse <- show_best(elastic_tune, n=1)$mean %>% round(2)

rf_rmse <- show_best(rf_tune, n=1)$mean %>% round(2)

boosted_rmse <- show_best(boosted_tune, n=1)$mean %>% round(2)

rmse_table <- data.frame('Linear Regression' = lm_rmse,
                         'KNN' = knn_rmse,
                         'Elastic' = elastic_rmse,
                         'Random Forest' = rf_rmse,
                         'Boosted Tree' = boosted_rmse)

rmse_table %>% gt() %>% 
  tab_header(
    title = "Best RMSE of Tuned Models"
  )
```

```{r}
autoplot(knn_tune, metric='rmse') + theme_polyvore() -> p1

autoplot(elastic_tune, metric='rmse') + theme_polyvore() -> p2

autoplot(rf_tune, metric='rmse') + theme_polyvore() -> p3

autoplot(boosted_tune, metric='rmse') + theme_polyvore() -> p4

grid.arrange(p1, p2, p3, p4, ncol=2)
```


```{r}

```

```{r}

```




